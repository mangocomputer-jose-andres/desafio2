{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# MARAT\u00d3N BEHIND THE CODE 2020\n\n## DESAF\u00cdO 2: PARTE 1"}, {"metadata": {}, "cell_type": "markdown", "source": "### Introducci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "En proyectos de ciencia de datos destinados a construir modelos de *aprendizaje autom\u00e1tico*, o aprendizaje estad\u00edstico, es muy inusual que los datos iniciales ya est\u00e9n en el formato ideal para la construcci\u00f3n de modelos. Se requieren varios pasos intermedios de preprocesamiento de datos, como la codificaci\u00f3n de variables categ\u00f3ricas, normalizaci\u00f3n de variables num\u00e9ricas, tratamiento de datos faltantes, etc. La biblioteca **scikit-learn**, una de las bibliotecas de c\u00f3digo abierto m\u00e1s populares para *aprendizaje autom\u00e1tico* en el mundo, ya tiene varias funciones integradas para realizar las transformaciones de datos m\u00e1s utilizadas. Sin embargo, en un flujo com\u00fan de un modelo de aprendizaje autom\u00e1tico, es necesario aplicar estas transformaciones al menos dos veces: la primera vez para \"entrenar\" el modelo, y luego nuevamente cuando se env\u00edan nuevos datos como entrada para ser clasificados por este modelo.\n\nPara facilitar el trabajo con este tipo de flujos, scikit-learn tambi\u00e9n cuenta con una herramienta llamada **Pipeline**, que no es m\u00e1s que una lista ordenada de transformaciones que se deben aplicar a los datos. Para ayudar en el desarrollo y la gesti\u00f3n de todo el ciclo de vida de estas aplicaciones, adem\u00e1s del uso de Pipelines, los equipos de cient\u00edficos de datos pueden utilizar en conjunto **Watson Machine Learning**, que tiene docenas de herramientas para entrenar , gestionar, alojar y evaluar modelos basados \u200b\u200ben el aprendizaje autom\u00e1tico. Adem\u00e1s, Watson Machine Learning es capaz de encapsular pipelines y modelos en una API lista para usar e integrarse con otras aplicaciones.\n\nDurante el desaf\u00edo 2, aprender\u00e1s como crear un **Pipeline** para un modelo de clasificaci\u00f3n y alojarlo como una API con la ayuda de Watson Machine Learning. Una vez alojado, puedes integrar el modelo creado con otras aplicaciones, como asistentes virtuales y m\u00e1s. En este notebook, se presentar\u00e1 un ejemplo funcional de creaci\u00f3n de un modelo y un pipeline en scikit-learn (\u00a1que puedes usar como plantilla para tu soluci\u00f3n!)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Trabajando con Pipelines del scikit-learn"}, {"metadata": {}, "cell_type": "code", "source": "# Primero, realizamos la instalaci\u00f3n de scikit-learn versi\u00f3n 0.20.0 en el Kernel de este notebook:\n!pip install scikit-learn==0.20.0 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# A continuaci\u00f3n importaremos varias bibliotecas que se utilizar\u00e1n:\n\n# Biblioteca para trabajar con JSON\nimport json\n\n# Biblioteca para realizar solicitudes HTTP\nimport requests\n\n# Biblioteca para exploraci\u00f3n y an\u00e1lisis de datos\nimport pandas as pd\n\n# Biblioteca con m\u00e9todos num\u00e9ricos y representaciones matriciales\nimport numpy as np\n\n# Biblioteca para construir un modelo basado en la t\u00e9cnica Gradient Boosting\nimport xgboost as xgb\n\n# Paquetes scikit-learn para preprocesamiento de datos\n# \"SimpleImputer\" es una transformaci\u00f3n para completar los valores faltantes en conjuntos de datos\nfrom sklearn.impute import SimpleImputer\n\n# Paquetes de scikit-learn para entrenamiento de modelos y construcci\u00f3n de pipelines\n# M\u00e9todo para separar el conjunto de datos en muestras de testes y entrenamiento\nfrom sklearn.model_selection import train_test_split\n# M\u00e9todo para crear modelos basados en \u00e1rboles de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\n# Clase para crear una pipeline de machine-learning\nfrom sklearn.pipeline import Pipeline\n\n# Paquetes scikit-learn para evaluaci\u00f3n de modelos\n# M\u00e9todos para la validaci\u00f3n cruzada del modelo creado\nfrom sklearn.model_selection import KFold, cross_validate", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Importar  un .csv a tu proyecto en IBM Cloud Pak for Data al Kernel de este notebook"}, {"metadata": {}, "cell_type": "markdown", "source": "Primero, importaremos el conjunto de datos proporcionado para el desaf\u00edo, que ya est\u00e1 incluido en este proyecto.\n\nPuedes importar datos desde un archivo .csv directamente al Kernel del port\u00e1til como un Pandas DataFrame, que se usa ampliamente para manipular datos en Python.\n\nPara realizar la importaci\u00f3n, simplemente selecciona la siguiente celda y siga las instrucciones en la imagen a continuaci\u00f3n:\n\n![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n\nDespu\u00e9s de seleccionar la opci\u00f3n **\"Insertar en el c\u00f3digo\"**, la celda de abajo se llenar\u00e1 con el c\u00f3digo necesario para importar y leer los datos en el archivo .csv como un Pandas DataFrame."}, {"metadata": {}, "cell_type": "code", "source": "\n<< inserte el DataFrame Pandas aqu\u00ed >>\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tenemos 16 columnas presentes en el set de datos proporcionado, 15 de las cuales son variables features (datos de entrada) y una de ellas es una variable target (que queremos que nuestro modelo va a predecir).\n\nLas variables features son:\n\n    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n    NAME                                - Nombre del estudiante\n    USER_ID                             - N\u00famero de identificaci\u00f3n del estudiante\n    HOURS_DATASCIENCE                   - N\u00famero de horas de estudio en Data Science\n    HOURS_BACKEND                       - N\u00famero de horas de estudio en Web (Back-End)\n    HOURS_FRONTEND                      - N\u00famero de horas de estudio en Web (Front-End)\n    NUM_COURSES_BEGINNER_DATASCIENCE    - N\u00famero de cursos de nivel principiante en Data Science completados por el estudiante\n    NUM_COURSES_BEGINNER_BACKEND        - N\u00famero de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n    NUM_COURSES_BEGINNER_FRONTEND       - N\u00famero de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_DATASCIENCE    - N\u00famero de cursos de nivel avanzado en Data Science completados por el estudiante\n    NUM_COURSES_ADVANCED_BACKEND        - N\u00famero de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_FRONTEND       - N\u00famero de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n    \nLa variable target es:\n\n    PROFILE                             - Perfil de carrera del estudiante (puede ser uno de 6)\n    \n        - beginner_front_end\n        - advanced_front_end\n        - beginner_back_end\n        - advanced_back_end\n        - beginner_data_science\n        - advanced_data_science\n        \nCon un modelo capaz de clasificar a un alumno en una de estas categor\u00edas, podemos recomendar contenidos a los alumnos de forma personalizada seg\u00fan las necesidades de cada alumno."}, {"metadata": {}, "cell_type": "markdown", "source": "### Explorando los datos proporcionados\n\nPodemos continuar la exploraci\u00f3n de los datos proporcionados con la funci\u00f3n ``info()``:"}, {"metadata": {}, "cell_type": "code", "source": "df_data_1.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Visualizaci\u00f3n (visualizations)\n\nPara ver el conjunto de datos suministrado, podemos usar las bibliotecas ``matplotlib`` y ``seaborn``:"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_data_1['HOURS_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_data_1['HOURS_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_data_1['HOURS_FRONTEND'].dropna(), ax=axes[2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_data_1['NUM_COURSES_BEGINNER_DATASCIENCE'].dropna(), ax=axes[0][0])\nsns.distplot(df_data_1['NUM_COURSES_BEGINNER_BACKEND'].dropna(), ax=axes[0][1])\nsns.distplot(df_data_1['NUM_COURSES_BEGINNER_FRONTEND'].dropna(), ax=axes[0][2])\nsns.distplot(df_data_1['NUM_COURSES_ADVANCED_DATASCIENCE'].dropna(), ax=axes[1][0])\nsns.distplot(df_data_1['NUM_COURSES_ADVANCED_BACKEND'].dropna(), ax=axes[1][1])\nsns.distplot(df_data_1['NUM_COURSES_ADVANCED_FRONTEND'].dropna(), ax=axes[1][2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_data_1['AVG_SCORE_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_data_1['AVG_SCORE_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_data_1['AVG_SCORE_FRONTEND'].dropna(), ax=axes[2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(28, 4))\n\nsns.countplot(ax=axes[0], x='PROFILE', data=df_data_1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Preprocesamiento de datos"}, {"metadata": {}, "cell_type": "markdown", "source": "Para el preprocesamiento de los datos, se presentar\u00e1n en este notebook dos transformaciones b\u00e1sicas, demostrando la construcci\u00f3n de un Pipeline con un modelo funcional. Este Pipeline funcional provisto debe ser mejorado por el participante para que el modelo final alcance la mayor precisi\u00f3n posible, garantizando una mayor puntuaci\u00f3n en el desaf\u00edo. Esta mejora solo se puede realizar en el preprocesamiento de los datos, en la elecci\u00f3n de un algoritmo para el entrenamiento de diferentes modelos, o incluso en la alteraci\u00f3n del **framework** utilizado (sin embargo, solo se entregar\u00e1 un ejemplo de integraci\u00f3n de Watson Machine Learning con *scikit-learn*).\n\nLa primera transformaci\u00f3n (paso en nuestro Pipeline) ser\u00e1 la exclusi\u00f3n de la columna \"NOMBRE\" de nuestro conjunto de datos, que adem\u00e1s de no ser una variable num\u00e9rica, tampoco es una variable relacionada con el desempe\u00f1o de los estudiantes en las disciplinas. Hay funciones listas para usar en *scikit-learn* para realizar esta transformaci\u00f3n, sin embargo, nuestro ejemplo demostrar\u00e1 c\u00f3mo crear una transformaci\u00f3n personalizada desde cero en scikit-learn. Si lo desea, el participante puede usar este ejemplo para crear otras transformaciones y agregarlas al Pipeline final :)"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Transformaci\u00f3n 1: excluir columnas del conjunto de datos\n\nPara la creaci\u00f3n de una transformaci\u00f3n de datos personalizada en scikit-learn, es necesario crear una clase con los m\u00e9todos ``transform`` y ``fit``. En el m\u00e9todo de 'transform', se ejecutar\u00e1 la l\u00f3gica de nuestra transformaci\u00f3n.\n\nLa siguiente celda muestra el c\u00f3digo completo de una transformaci\u00f3n ``DropColumns`` para eliminar columnas de un pandas DataFrame."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\n\n# All sklearn Transforms must have the `transform` and `fit` methods\nclass DropColumns(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Primero copiamos el dataframe de datos de entrada 'X'\n        data = X.copy()\n        # Devolvemos un nuevo dataframe de datos sin las columnas no deseadas\n        return data.drop(labels=self.columns, axis='columns')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Para aplicar esa transformaci\u00f3n en un pandas DataFrame pandas, basta instanciar un objeto *DropColumns* y llamar el m\u00e9todo transform()."}, {"metadata": {}, "cell_type": "code", "source": "# Creaci\u00f3n de instancias de una transformaci\u00f3n DropColumns\nrm_columns = DropColumns(\n    columns=[\"NAME\", \"Unnamed: 0\"]  # Esta transformaci\u00f3n toma como par\u00e1metro una lista con los nombres de las columnas no deseadas\n)\n\nprint(rm_columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver las columnas del conjunto de datos original\nprint(\"Columnas del conjunto de datos original: \\n\")\nprint(df_data_1.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Aplicar la transformaci\u00f3n ``DropColumns`` al conjunto de datos base\nrm_columns.fit(X=df_data_1)\n\n# Reconstruyendo un DataFrame de Pandas con el resultado de la transformaci\u00f3n\ndf_data_2 = pd.DataFrame.from_records(\n    data=rm_columns.transform(\n        X=df_data_1\n    ),\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver las columnas del conjunto de datos transformado\nprint(\"Columnas del conjunto de datos despu\u00e9s de la transformaci\u00f3n ``DropColumns``: \\n\")\nprint(df_data_2.columns)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tenga en cuenta que la columna \"NOMBRE\" se ha eliminado y nuestro conjunto de datos ahora solo tiene 14 columnas."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Transformaci\u00f3n 2: tratamiento de datos faltantes\n\nPara manejar los datos que faltan en nuestro conjunto de datos, ahora usaremos una transformaci\u00f3n lista para usar de la biblioteca scikit-learn, llamada **SimpleImputer**.\n\nEsta transformaci\u00f3n permite varias estrategias para el tratamiento de datos faltantes. La documentaci\u00f3n oficial se puede encontrar en: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\nEn este ejemplo, simplemente haremos cero todos los valores faltantes."}, {"metadata": {}, "cell_type": "code", "source": "# Crear un objeto ``SimpleImputer``\nsi = SimpleImputer(\n    missing_values=np.nan,  # los valores que faltan son del tipo ``np.nan`` (Pandas est\u00e1ndar)\n    strategy='constant',  # la estrategia elegida es cambiar el valor faltante por una constante\n    fill_value=0,  # la constante que se usar\u00e1 para completar los valores faltantes es un int64 = 0\n    verbose=0,\n    copy=True\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver los datos faltantes del conjunto de datos antes de la primera transformaci\u00f3n (df_data_2)\nprint(\"Valores nulos antes de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df_data_2.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Aplicamos el SimpleImputer ``si`` al conjunto de datos df_data_2 (resultado de la primera transformaci\u00f3n)\nsi.fit(X=df_data_2)\n\n# Reconstrucci\u00f3n de un nuevo DataFrame de Pandas con el conjunto imputado (df_data_3)\ndf_data_3 = pd.DataFrame.from_records(\n    data=si.transform(\n        X=df_data_2\n    ),  # el resultado SimpleImputer.transform (<< pandas dataframe >>) es lista lista\n    columns=df_data_2.columns  # las columnas originales deben conservarse en esta transformaci\u00f3n\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Ver los datos faltantes del conjunto de datos despu\u00e9s de la segunda transformaci\u00f3n (SimpleImputer) (df_data_3)\nprint(\"Valores nulos en el conjunto de datos despu\u00e9s de la transformaci\u00f3n SimpleImputer: \\n\\n{}\\n\".format(df_data_3.isnull().sum(axis = 0)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Tenga en cuenta que ya no tenemos valores perdidos en nuestro conjunto de datos :)\n\nVale la pena se\u00f1alar que cambiar los valores perdidos por 0 no siempre es la mejor estrategia. Se anima al participante a estudiar e implementar diferentes estrategias para tratar los valores perdidos para mejorar su modelo y mejorar su puntuaci\u00f3n final."}, {"metadata": {}, "cell_type": "markdown", "source": "### Entrenando un modelo de clasificaci\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "Una vez finalizado el preprocesamiento, ya tenemos el conjunto de datos en el formato necesario para entrenar nuestro modelo:"}, {"metadata": {}, "cell_type": "code", "source": "df_data_3.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "En el ejemplo proporcionado, usaremos todas las columnas, excepto la columna **Profile** como *feautres* (variables de entrada).\n\nLa variable **Profile** ser\u00e1 la variable objetivo del modelo, como se describe en la declaraci\u00f3n de desaf\u00edo."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Definici\u00f3n de features del modelo"}, {"metadata": {}, "cell_type": "code", "source": "# Definici\u00f3n de las columnas que seran features (Notese que la columna NOMBRE no esta presente)\nfeatures = [\n    \"USER_ID\", \"HOURS_DATASCIENCE\", \"HOURS_BACKEND\", \"HOURS_FRONTEND\",\n    \"NUM_COURSES_BEGINNER_DATASCIENCE\", \"NUM_COURSES_BEGINNER_BACKEND\", \"NUM_COURSES_BEGINNER_FRONTEND\",\n    \"NUM_COURSES_ADVANCED_DATASCIENCE\", \"NUM_COURSES_ADVANCED_BACKEND\", \"NUM_COURSES_ADVANCED_FRONTEND\",\n    \"AVG_SCORE_DATASCIENCE\", \"AVG_SCORE_BACKEND\", \"AVG_SCORE_FRONTEND\"\n]\n\n# Definici\u00f3n de variable objetivo\ntarget = ['PROFILE']\n\n# Preparaci\u00f3n de los argumentos para los m\u00e9todos de la biblioteca ``scikit-learn``\nX = df_data_3[features]\ny = df_data_3[target]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "El conjunto de entrada (X):"}, {"metadata": {}, "cell_type": "code", "source": "X.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "La variable objetivo (y):"}, {"metadata": {}, "cell_type": "code", "source": "y.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Separar el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba"}, {"metadata": {}, "cell_type": "markdown", "source": "Separaremos el conjunto de datos provisto en dos grupos: uno para entrenar nuestro modelo y otro para probar el resultado a trav\u00e9s de una prueba ciega. La separaci\u00f3n del conjunto de datos se puede hacer f\u00e1cilmente con el m\u00e9todo *train_test_split ()* de scikit-learn:"}, {"metadata": {}, "cell_type": "code", "source": "# Separaci\u00f3n de datos en conjunto de entrenamiento y conjunto de pruebas\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Creando un modelo basado en \u00e1rboles de decisi\u00f3n"}, {"metadata": {}, "cell_type": "markdown", "source": "En el ejemplo proporcionado, crearemos un clasificador basado en **\u00e1rboles de decisi\u00f3n**.\n\nEl primer paso es b\u00e1sicamente crear una instancia de un objeto *DecisionTreeClassifier ()* de la biblioteca scikit-learn."}, {"metadata": {}, "cell_type": "code", "source": "# Creando el \u00e1rbol de decisiones con la biblioteca ``scikit-learn``:\ndtc_model = DecisionTreeClassifier()  # El modelo se crear\u00e1 con los par\u00e1metros est\u00e1ndar de la biblioteca", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Material te\u00f3rico sobre \u00e1rboles de decisi\u00f3n en la documentaci\u00f3n oficial de scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n\nUna gu\u00eda para principiantes del mundo del aprendizaje autom\u00e1tico: https://developer.ibm.com/es/patterns/use-icp4d-to-build-the-machine-learning-model-for-return-propensity/"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Ejecucion del entrenamiento del \u00e1rbol de descisi\u00f3n "}, {"metadata": {}, "cell_type": "code", "source": "# Entrenamiento de modelos (llamado m\u00e9todo *fit ()* con conjuntos de entrenamiento)\ndtc_model.fit(\n    X_train,\n    y_train\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Ejecuci\u00f3n de predicciones y evaluaci\u00f3n del modelo creado"}, {"metadata": {}, "cell_type": "code", "source": "# Realizaci\u00f3n de una prueba a ciegas en el modelo creado\ny_pred = dtc_model.predict(X_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_test.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(y_pred)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import accuracy_score\n\n# Precisi\u00f3n lograda por el \u00e1rbol de decisiones\nprint(\"Exactitud: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "Este notebook demostr\u00f3 c\u00f3mo trabajar con transformaciones y modelos con la biblioteca scikit-learn. Se recomienda que el participante realice sus experimentos editando el c\u00f3digo proporcionado aqu\u00ed hasta lograr un modelo con alta precisi\u00f3n.\n\nCuando est\u00e9 satisfecho con su modelo, puede pasar al segundo paso del desaf\u00edo: encapsular su modelo como una API REST lista para usar con Watson Machine Learning.\n\nEl notebook para la segunda etapa ya est\u00e1 en este proyecto, simplemente acceda a la pesta\u00f1a **ASSETS** e in\u00edcielo. No olvide apagar primero el Kernel en este port\u00e1til para reducir el consumo de su nivel gratuito de IBM Cloud Pak for Data."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}